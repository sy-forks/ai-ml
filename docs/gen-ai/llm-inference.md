# LLM Inference & Serving

## LLM vLLM

- [vLLM v0.6.0: 2.7x Throughput Improvement and 5x Latency Reduction](https://blog.vllm.ai/2024/09/05/perf-update.html)
- [vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)

## Huggingface TGI

- [Benchmarking Text Generation Inference](https://huggingface.co/blog/tgi-benchmarking)
- [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/en/index)

